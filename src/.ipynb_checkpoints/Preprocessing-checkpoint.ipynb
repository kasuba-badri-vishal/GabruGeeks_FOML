{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb36e25",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13d1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ed5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64392d",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff3654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan_tags(dataframe):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments \n",
    "        dataframe : Pandas dataFrame\n",
    "    Function:\n",
    "        Removes nan values present as float in the dataset\n",
    "        Does inplce operation on given dataset\n",
    "    Returns\n",
    "        Nothing\n",
    "    \n",
    "    \"\"\"  \n",
    "    tags = df.loc[:, 'Tags']\n",
    "    tags = tags.to_numpy()\n",
    "    \n",
    "    nonstrArr = []                            # Non String array\n",
    "    for index,tag in enumerate(tags):\n",
    "        if not(isinstance(tag, str)):         # If data type is not a string\n",
    "            nonstrArr.append([index,tag])\n",
    "    \n",
    "    nonstrArr = np.array(nonstrArr)           # change to numpy array\n",
    "    nonstrArr = nonstrArr[:,0]\n",
    "    to_remove =list(map(lambda x: int(x) ,nonstrArr)) # typecast as int\n",
    "    \n",
    "    dataframe = dataframe.drop(to_remove, axis=0, inplace=True) \n",
    "    dataframe = dataframe.reset_index(drop=True, inplace=True)\n",
    "    return dataframe\n",
    "   \n",
    "\n",
    "def topn_tags(dataFrame ,n):\n",
    "    \"\"\"\n",
    "    This function returns a list of top n tags \n",
    "    :param dataFrame: pandas dataFrame\n",
    "    :param n: integer\n",
    "    :returns: list of strings\n",
    "    \"\"\"\n",
    "    dataFrame['Tags'] = dataFrame['Tags'].apply(lambda x : x.split(' '))\n",
    "    \n",
    "    counter = Counter()\n",
    "    _ = df['Tags'].apply(counter.update)\n",
    "    \n",
    "    unique_tags = counter.most_common(n)       # Top n tags\n",
    "    tags_df = pd.DataFrame(unique_tags, columns=['Tags','Freq'])\n",
    "    unique_tags = tags_df.Tags.tolist()\n",
    "    \n",
    "    return unique_tags\n",
    "    \n",
    "\n",
    "def separate_code_and_body(body):\n",
    "    code_snippets = re.finditer(\"<code.*?>(.*?)</code>\", body, re.DOTALL)\n",
    "    code = []\n",
    "    description = body\n",
    "    for snip in code_snippets:\n",
    "        code.append(snip.group())\n",
    "        body = body.replace(snip.group(),\"\")\n",
    "    return [code, body]  \n",
    "\n",
    "\n",
    "def convert_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r\"[^A-Za-z ]+\",'', text)\n",
    "\n",
    "def get_intersection(x, y):\n",
    "    return list(set(x) & set(y))\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean,'', text)\n",
    "\n",
    "def remove_n(text):\n",
    "    \"\"\"Remove \\n tags from a string\"\"\"\n",
    "    clean1 = re.compile('\\n')\n",
    "    return re.sub(clean1,'', text)\n",
    "\n",
    "def string_to_tokens(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def filter_tokens(text):\n",
    "    text_token=string_to_tokens(text)\n",
    "    filtered_sentence=[]\n",
    "    for w in text_token:\n",
    "        if w not in stop_words or w=='c':\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc59c627",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2522f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/processed/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c0dac",
   "metadata": {},
   "source": [
    "### Data Pre - Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0a0a3",
   "metadata": {},
   "source": [
    "#### Tags Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ca337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nan Tag values\n",
    "dropnantags(df)\n",
    "\n",
    "# Filtering to Top 100 tags on Frequency\n",
    "unique_tags = topn_tags(df,100)\n",
    "\n",
    "# Filtering the Dataset for top 100 tags\n",
    "df['Tags'] = df['Tags'].apply(lambda x : get_intersection(x,unique_tags))\n",
    "df = df[df['Tags'].map(lambda d: len(d)) > 0]\n",
    "df = df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd018081",
   "metadata": {},
   "source": [
    "#### Separate Code from Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8b121b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 45250.78it/s]\n"
     ]
    }
   ],
   "source": [
    "df['Code'] = df['Body'].progress_apply(lambda x : separate_code_and_body(x))\n",
    "df[['Code','Description']] = pd.DataFrame(df.Code.tolist(), index= df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e721c",
   "metadata": {},
   "source": [
    "#### Convert to Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "fb664ab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 365131.67it/s]\n",
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 313363.22it/s]\n",
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 501543.02it/s]\n"
     ]
    }
   ],
   "source": [
    "df['Title'] = df['Title'].progress_apply(lambda x:  convert_to_lower(x))\n",
    "df['Description'] = df['Description'].progress_apply(lambda x:  convert_to_lower(x))\n",
    "df['Tags'] = df['Tags'].progress_apply(lambda x: convert_to_lower(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bc238",
   "metadata": {},
   "source": [
    "#### Description Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df['Description'].progress_apply(lambda x: remove_html_tags(x))\n",
    "df['Description'] = df['Description'].progress_apply(lambda x: remove_n(x))\n",
    "df['Description'] = df['Description'].progress_apply(lambda x: remove_special_chars(x))\n",
    "df['Description_Tokens'] = df['Description'].progress_apply(lambda x: filter_tokens(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
